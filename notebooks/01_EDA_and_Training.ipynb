{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "494011b5-2e9a-4a50-96d7-2fef906bc426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Dataset Preview:\n",
      "                                         video_title  \\\n",
      "0                     BTS: Boy with Luv (Live) - SNL   \n",
      "1          Star Wars: The Rise of Skywalker â€“ Teaser   \n",
      "2  Gordon Ramsay Enters An Indian Cooking Competi...   \n",
      "3                         We Got Married...(Pt. 2/4)   \n",
      "4             BTS Eat Churros on The Morning Mash Up   \n",
      "\n",
      "                                   video_description  label  \\\n",
      "0  Musical guest BTS performs \"Boy with Luv\" on S...      1   \n",
      "1  Every generation has a legend. Watch the brand...      1   \n",
      "2  As Gordon's trip in Malaysia comes towards an ...      1   \n",
      "3  The Day i Committed To My Bestfriend!!\\n\\n\\nFO...      1   \n",
      "4  The Morning Mash Up crew gifted BTS with their...      1   \n",
      "\n",
      "                                                text  \n",
      "0  BTS: Boy with Luv (Live) - SNL Musical guest B...  \n",
      "1  Star Wars: The Rise of Skywalker â€“ Teaser Ever...  \n",
      "2  Gordon Ramsay Enters An Indian Cooking Competi...  \n",
      "3  We Got Married...(Pt. 2/4) The Day i Committed...  \n",
      "4  BTS Eat Churros on The Morning Mash Up The Mor...  \n",
      "Total samples: 405511\n",
      "Trending videos: 404464\n",
      "Non-trending videos: 1047\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the datasets\n",
    "df_trending = pd.read_csv(\"../data/trending.csv\")\n",
    "df_non_trending = pd.read_excel(\"../data/non_trending.xlsx\")\n",
    "\n",
    "# Step 2: Add label columns\n",
    "df_trending['label'] = 1  # trending videos\n",
    "df_non_trending['label'] = 0  # non-trending videos\n",
    "\n",
    "# Step 3: Keep only needed columns and standardize names\n",
    "df_trending = df_trending[['video_title', 'video_description', 'label']]\n",
    "df_non_trending = df_non_trending[['title', 'description', 'label']]\n",
    "df_non_trending.rename(columns={'title': 'video_title', 'description': 'video_description'}, inplace=True)\n",
    "\n",
    "# Step 4: Combine the two DataFrames\n",
    "df_combined = pd.concat([df_trending, df_non_trending], ignore_index=True)\n",
    "\n",
    "# Step 5: Drop rows with missing title/description\n",
    "df_combined.dropna(subset=['video_title', 'video_description'], inplace=True)\n",
    "\n",
    "# Step 6: Create a combined 'text' column for model input\n",
    "df_combined['text'] = df_combined['video_title'] + \" \" + df_combined['video_description']\n",
    "\n",
    "# Step 7: Preview the result\n",
    "print(\"Combined Dataset Preview:\")\n",
    "print(df_combined.head())\n",
    "\n",
    "# Optional: Check how many samples you have\n",
    "print(\"Total samples:\", len(df_combined))\n",
    "print(\"Trending videos:\", df_combined['label'].sum())\n",
    "print(\"Non-trending videos:\", len(df_combined) - df_combined['label'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fd24fba-957e-4a4b-89af-58dcf3f34fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the combined DataFrame to a CSV file\n",
    "df_combined.to_csv(\"../data/combined_entertainment_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b9332c5-aaae-4fb7-879c-8776a27f43af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… TF-IDF matrix shape: (405511, 5000)\n",
      "âœ… Labels shape: (405511,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Step 1: Load the combined CSV (if not already loaded)\n",
    "df_combined = pd.read_csv(\"../data/combined_entertainment_data.csv\")\n",
    "\n",
    "# Step 2: Define text cleaning function\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()  # lowercase\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)  # remove URLs\n",
    "    text = re.sub(r'@\\w+|#', '', text)  # remove @mentions and hashtags\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # remove special characters\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # remove extra spaces\n",
    "    return text\n",
    "\n",
    "# Step 3: Clean the text column\n",
    "df_combined['clean_text'] = df_combined['text'].apply(clean_text)\n",
    "\n",
    "# Step 4: Prepare feature and label variables\n",
    "X_text = df_combined['clean_text']\n",
    "y = df_combined['label']\n",
    "\n",
    "# Step 5: Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words='english',       # remove common English stopwords\n",
    "    max_features=5000,          # limit to top 5000 features\n",
    "    ngram_range=(1, 2)          # use unigrams and bigrams\n",
    ")\n",
    "\n",
    "# Step 6: Fit and transform text into vector format\n",
    "X = vectorizer.fit_transform(X_text)\n",
    "\n",
    "# Step 7: Output shapes\n",
    "print(\"âœ… TF-IDF matrix shape:\", X.shape)\n",
    "print(\"âœ… Labels shape:\", y.shape)\n",
    "# Save the cleaned dataset with clean_text column\n",
    "df_combined.to_csv(\"../data/cleaned_entertainment_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f5553b2-c048-431a-aff5-23f285441008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "1    404464\n",
      "0      1047\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_combined['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e81f3b-07d2-4866-a924-e2adc65a87af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "# Load your cleaned dataset\n",
    "df = pd.read_csv(\"../data/cleaned_entertainment_data.csv\")\n",
    "\n",
    "# Check class distribution\n",
    "label_0 = df[df['label'] == 0]\n",
    "label_1 = df[df['label'] == 1]\n",
    "\n",
    "print(f\"Label 0 count: {len(label_0)}, Label 1 count: {len(label_1)}\")\n",
    "\n",
    "# Augmenter: Synonym replacement using WordNet\n",
    "aug = naw.SynonymAug(aug_src='wordnet', aug_min=1, aug_max=5)\n",
    "\n",
    "# Number of synthetic samples to generate\n",
    "num_augmented = 2000 - len(label_0)\n",
    "\n",
    "print(f\"ðŸ”„ Generating {num_augmented} synthetic samples for label=0...\")\n",
    "\n",
    "augmented_texts = []\n",
    "for i in range(num_augmented):\n",
    "    original_text = random.choice(label_0[\"clean_text\"].values)\n",
    "    try:\n",
    "        augmented = aug.augment(original_text)\n",
    "        augmented_texts.append(augmented)\n",
    "    except:\n",
    "        # In case augmentation fails, just use the original (you can improve this later)\n",
    "        augmented_texts.append(original_text)\n",
    "\n",
    "# Create synthetic DataFrame\n",
    "synthetic_df = pd.DataFrame({\n",
    "    \"video_title\": [\"synthetic_title\"] * num_augmented,\n",
    "    \"video_description\": [\"synthetic_description\"] * num_augmented,\n",
    "    \"text\": [\"synthetic_text\"] * num_augmented,\n",
    "    \"clean_text\": augmented_texts,\n",
    "    \"label\": [0] * num_augmented\n",
    "})\n",
    "\n",
    "# Combine original label=0 and synthetic to make 2,000\n",
    "combined_label_0 = pd.concat([label_0, synthetic_df], ignore_index=True).sample(n=2000, random_state=42)\n",
    "\n",
    "# Downsample label=1 to 2000\n",
    "label_1_downsampled = label_1.sample(n=2000, random_state=42)\n",
    "\n",
    "# Final balanced dataset\n",
    "balanced_df = pd.concat([combined_label_0, label_1_downsampled], ignore_index=True).sample(frac=1, random_state=42)\n",
    "\n",
    "# Save to CSV\n",
    "balanced_df.to_csv(\"../data/balanced_data_2000_augmented.csv\", index=False)\n",
    "\n",
    "print(f\"âœ… Synthetic data added. Final dataset shape: {balanced_df.shape}\")\n",
    "print(\"âœ… Saved as 'balanced_data_2000_augmented.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bba5f3b-76d0-411e-93a7-e69cf26a8527",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
